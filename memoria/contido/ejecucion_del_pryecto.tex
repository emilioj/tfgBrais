\chapter{Ejecución del proyecto}
\label{chap:edp}
Este capítulo tiene como objetivo tratar el desarrollo del proyecto en sí mismo, así como discutir las opciones disponibles durante el progreso y las decisiones tomadas para llevarlo a cabo.
%\section{Consideraciones previas}
%Este trabajo nace como un desarrollo del proyecto troncal de \citeauthor{IglesiasGuitian2022} y como tal se debe ceñir a las condiciones que acarrea dicho proyecto.
%Todo el equipo utilizado durante el desarrollo fue provisto por parte del mismo, o en su defecto por parte del \acrfull{citic}.
%\section{Dificultades}
%Dada la naturaleza del trabajo (altamente dependiente del hardware para su ejecución), es necesaria la presencialidad a lo largo de gran parte del desarrollo. Por causa mayor me he visto obligado a desplazarme al otro extremo de la península, lo que ha condicionado en parte el final del proyecto.
\section{Análisis}
Se llevó a cabo un estudio para definir la hoja de ruta del proyecto. Dada la problemática a solventar, este trabajo alcanza a tocar áreas bien diferenciadas entre si que se pueden destacar como los pasos a seguir del mismo:
\begin{itemize}
    \item Extracción de volúmenes 3D a partir de un \acrshort{tc} válidos para su impresión.
    \item Diseñar un marcador fiduciario que permita el seguimiento de una pieza en 3 dimensiones y un método de acople al volumen previamente impreso.
    \item Implementar una solución que permita el seguimiento de dicho marcador.
    \item Integrar la solución sobre un \acrshort{hmd}.
\end{itemize}

Fruto de la investigación surge el artículo de  \citeauthor{MoretaMartinez2020}; que expone una solución existente a los objetivos de este trabajo mediante el uso de software bajo licencia o de pago.  Es por ello que se toma una aproximación similar al problema, sobre todo en las fases iniciales, para la generación de los volúmenes a pesar de implementar una solución propia para lo que a seguimiento se refiere. Este estudio también permitió especificar los requisitos necesarios para el software de tracking.

Uno de los principales requisitos debe ser la robustez del sistema frente a las oclusiones del marcador. Es necesario que el seguimiento sea posible a pesar de la oclusión parcial del marcador. Además, es preciso que a partir de una fuente de vídeo se puedan extraer las coordenadas del marcador, así como su rotación en el espacio. Destacar también que el seguimiento debe ocurrir en un segundo plano, entorpeciendo lo menos posible las operaciones del hilo principal de ejecución, ya que parte de estas operaciones tiene una latencia crítica.

Este trabajo nace como un desarrollo del proyecto troncal de \citeauthor{IglesiasGuitian2022}, y como tal, debe ceñirse a ciertas pautas del mismo. Exposure Render cuenta con un módulo de realidad virtual, en el que se integrará la solución de tracking para lograr un control mas natural sobre el modelo a tratar, moviéndose en las imágenes renderizadas a la par que se mueve en la realidad, como se muestra en el diagrama de secuencia de la \figurename~\ref{fig:flow_exposure}.

\begin{figure}
  \centering
  \includesvg[width=.75\textwidth]{imaxes/flow_exposure.svg}
  \caption{Flujo de integración del seguimiento en Exposure Render.}
  \label{fig:flow_exposure}
\end{figure}

\section{Generación de volúmenes a partir de TC}
Con el fin de facilitar la validación del progreso del proyecto, se utilizó una \acrshort{tc} de pruebas. Dichos datos contienen la sección superior (hombros y cabeza) de un sujeto, mostrado en la \figurename~\ref{fig:manix_full}. Durante el desarrollo se sugirió como posible caso práctico seleccionar el cráneo del sujeto en los datos de prueba y trabajar en la alineación 3D sobre el mismo.
%En las siguientes figuras se representa este objetivo.

% \begin{figure}
%   \centering
%   \includegraphics[width=0.4\textwidth]{imaxes/manix_full.png}
%   \caption{Datos de prueba.}
%   \label{fig:manix_full}
% \end{figure}

Con el fin de seleccionar una sección concreta para exportar, se utilizaron las herramientas para segmentar volúmenes de Slicer3D.
Al abrir el programa se pueden ver las vistas, en las que se representará el \acrshort{tc} una vez cargado, como se muestra en la captura de pantalla de la \figurename~\ref{fig:3dslier}.
Se utilizo principalmente la herramienta de ``Thresholding'' que permite seleccionar partes del modelo cuyas intensidades se comprenden en un intervalo o ``threshold'' (ver \figurename~\ref{fig:seg_cr}). Posteriormente, para la eliminación de las partes del modelo no deseadas, se utilizó la herramienta de borrado hasta alcanzar el volumen deseado.

\begin{figure}
  \centering
  \begin{subfigure}{.4\textwidth}
    \includegraphics[width=\textwidth]{imaxes/manix_full.png}
    \caption{TC usado en el desarrollo.}\label{fig:manix_full}
  \end{subfigure}
  \begin{minipage}[b]{.47\textwidth}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{imaxes/captura3dslicer_2.png}
      \caption{ Captura de pantalla de 3DSlicer con los datos cargados.}\label{fig:3dslier}
    \end{subfigure}\\
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{imaxes/segment_craneo.png}
      \caption{ Captura de pantalla de 3DSlicer del torso una vez aplicado el thresholding.}\label{fig:seg_cr}
    \end{subfigure}
  \end{minipage}
  \caption{\acrshort{tc} de partida y obtención del modelo 3D final con 3DSlicer.}
\end{figure}

% \begin{figure}%
%     \centering
%     \subfloat[\centering Captura de pantalla de 3DSlicer antes de cargar los datos.]{{\includegraphics[width=6cm]{imaxes/captura3dslicer_1.png} }}%
%     \qquad
%     \subfloat[\centering  Captura de pantalla de 3DSlicer con los datos cargados.]{{\includegraphics[width=6cm]{imaxes/captura3dslicer_2.png} }}%
%     \caption{Capturas de 3DSlicer}%
%     \label{fig:3dslier}%
% \end{figure}

% \begin{figure}[hp!]
%   \centering
%   \includegraphics[width=0.75\textwidth]{imaxes/segment_craneo.png}
%   \caption{Torso una vez aplicado el thresholding.}
%   \label{fig:seg_cr}
% \end{figure}

\section{Impresión 3D del volumen}
 Dado que el modelo no se genera a partir de figuras geométricas previas, es posible que presente geometrías rotas. Las cuales a la hora del Slicing provocarían errores y no permitirían que se genere el GCODE correctamente.
 Por ello es necesario importar el modelo en un programa que nos facilite arreglar estas geometrías como es Meshmixer. En la \figurename~\ref{fig:arr_geo} se aprecia el modelo exportado, y cada marcador corresponde a errores producto de la exportación. Una vez reparados, se procede a imprimir la pieza.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{imaxes/arreglo_geo.png}
  \caption{Modelo de cráneo 3D con geometrías erróneas.}
  \label{fig:arr_geo}
\end{figure}

 Como se comenta en el Capítulo \ref{chap:hs}, para una pieza con una geometría tan compleja, se requeriría una gran cantidad de soportes. Debido a esto, se optó por la impresora Fuse 1 para la impresión de este modelo. A diferencia de una impresora 3D al uso, esta impresora utiliza un láser para fijar capa a capa el polvo de nylon, lo que garantiza una gran resolución en la pieza final y una gran durabilidad de la misma. 

 Posterior al  trabajo de impresión es necesario retirar el material sobrante en la cámara de recuperación que cuenta con distintos utensilios para evitar malgastar el material sobrante ya que puede ser reutilizado.

\begin{figure}%
    \centering
    \subfloat[\centering Pieza en el proceso de recuperación del material.]{{\includegraphics[width=.4\textwidth]{imaxes/limpiando_fig.png} }}%
    \qquad
    \subfloat[\centering  Pieza final.]{{\includegraphics[width=.4\textwidth]{imaxes/limpia_fig.png} }}%
    \caption{Proceso de recuperación de material.}%
    \label{fig:3dslier}%
\end{figure}

\section{Desarrollo del marcador fiduciario}
Obtener la posición y rotación de una figura desconocida en el espacio es uno de los problemas principales a la hora de implementar soluciones de realidad virtual o aumentada, ya que requiere encontrar correspondencias entre objetos conocidos en el espacio y sus proyecciones en el vídeo.

Si bien existen aproximaciones que buscan puntos claves de las figuras o reconocen sus geometrías mediante técnicas de visión artificial e inteligencia artificial, se optó por el uso de  marcadores fiduciarios por varios motivos.

Primeramente, permite replicar el seguimiento del objeto independientemente del hardware utilizado, ya que una vez calibrada la cámara no se requiere ningún otro tipo de ajuste en el sistema. Otra ventaja es la robustez del sistema, ya que permite mantener el seguimiento a pesar de que parte del marcador se encuentre ocluido o no esté en el campo de visión de la cámara.
Dados los recursos disponibles, se optó por utilizar la librería ARuco para generar y seguir el marcador.

ARuco a la hora de detectar la posición de un marcador, trabaja con un modelo de coordenadas pin-hole, donde las coordenadas y rotación de los objetos detectados se expresan en función de la posición de la cámara.
El calibrado de la cámara permite determinar la proyección de cualquier punto en las 3 dimensiones del espacio en el sensor de la cámara. En una cámara ideal, un punto 3D $(X, Y, Z)$ en el espacio se proyectaría en el píxel:
\begin{align*}
x &= \frac{X \cdot fx}{Z} + cx & y = \frac{Y \cdot fy}{Z} + cy
\end{align*}
Donde:
\begin{itemize}
\item $fx$, $fy$: Es la longitud focal de la lente de la cámara en ambos ejes.
\item $cx$, $cy$: Es el centro óptico del sensor (expresado en píxeles).
\item $k1$, $k2$, $p1$, $p2$, $k3$: Son los coeficientes de distorsión.
\end{itemize}
Asumiendo que la ubicación tridimensional del punto con respecto al sistema de referencia de la cámara es conocida. Si se desea conocer la proyección de un punto referido a un sistema de referencia arbitrario, entonces deben mencionarse parámetros extrínsecos. Los parámetros extrínsecos consisten básicamente en las rotaciones tridimensionales (Rvec = {Rx, Ry, Rz}) y las traslaciones tridimensionales (Tvec = {Tx, Ty, Tz}) requeridas para trasladar el sistema de referencia de la cámara al sistema arbitrario. Los elementos de rotación se expresan mediante la fórmula de Rodrigues \cite{mebius2007derivation}, por lo que es posible obtener la matriz de rotación equivalente de 3x3 utilizando la función cv::Rodrigues() de OpenCV.

Cada marcador detectado devuelve como coordenadas la esquina superior izquierda del mismo, o lo que se etiqueta en el ejemplo de la \figurename~\ref{fig:marker_schema} como \emph{corner 0}, en forma de (Rvec = {Rx, Ry, Rz}) como vector de rotación y (Tvec = {Tx, Ty, Tz}) como vector de translación.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{imaxes/marker_schema.png}
  \caption{Esquema de un marcador.}
  \label{fig:marker_schema}
\end{figure}

Detectar un solo marcador puede fallar por diferentes razones, como malas condiciones de iluminación, movimiento rápido de la cámara, obstrucciones, etc. Para superar ese problema, ArUco permite el uso de tablas de marcadores como la mostrada en la \figurename~\ref{fig:board_schema}. Cada tabla de marcadores está compuesta por varios marcadores en ubicaciones conocidas. Presentan dos ventajas principales. Primero, dado que hay más de un marcador, es menos probable perderlos todos al mismo tiempo. Segundo, cuanto más marcadores se detecten, más puntos están disponibles para calcular los parámetros extrínsecos de la cámara. Como consecuencia, se obtiene una mayor precisión. 

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth, angle=90]{imaxes/board_schema.png}
  \caption{Esquema de una tabla de marcadores.}
  \label{fig:board_schema}
\end{figure}


Debido a la versatilidad de las piezas con las que se pretende usar el marcador, se implementó priorizando eliminar las oclusiones del marcador por la pieza, por este motivo se diseñó como un cubo, de forma que al menos una cara sería visible en todo momento. 

Se llevaron a cabo pruebas con distintos diccionarios de marcadores, modificando las tolerancias para los márgenes entre marcadores y los bordes del cubo con el fin de poder mantener unas dimensiones manejables sin comprometer el tamaño de cada marcador individual.
Finalmente, fruto de los tests, se llegó al cubo de la\figurename~\ref{fig:cubo_marker}, que cuenta con una matriz 2 x 2 de marcadores del mismo diccionario en cada cara.

El cubo se descompone como se puede ver en la \figurename~\ref{fig:cube_layout}. En un primer momento la detección devuelve la posición y rotación de cada cara de forma individual. Dado que se busca la posición del centro del cubo, sobre cada valor obtenido, se aplica una transformada para obtener el vector de translación del centro del cubo. Se calcula como la posición original menos la distancia del lado de media tabla  en cada eje, como se ve a continuación :

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{imaxes/cube_layout.png}
  \caption{Layout de las caras del marcador fiduciario.}
  \label{fig:cube_layout}
\end{figure}

\begin{lstlisting}[language=C++]
cv::Vec3d moveAxis(cv::Vec3d& tvec, cv::Vec3d rvec, double distance, int axis)
{
    cv::Mat rotationMatrix, rotationMatrixTransposed;
    Rodrigues(rvec, rotationMatrix);
    rotationMatrixTransposed = rotationMatrix.t();
    double* rz = rotationMatrixTransposed.ptr<double>(axis); // x=0, y=1, z=2
    tvec[0] -= rz[0] * distance;
    tvec[1] -= rz[1] * distance;
    tvec[2] -= rz[2] * distance;
    return tvec;
}
tvecs = moveAxis(tvecs, rvecs, -SIDELENGTH, 0);
tvecs = moveAxis(tvecs, rvecs, -SIDELENGTH, 1);
tvecs = moveAxis(tvecs, rvecs, -SIDELENGTH, 2);

\end{lstlisting}

No obstante, cada cara precisa de una rotación específica para mantener la rotación del cubo congruente con el resto de caras, excepto una que se tome como referencia:
\begin{lstlisting}[language=C++]


void cubeCoordinates(int id, cv::Vec3d& rvecs, cv::Vec3d& tvecs, float sideLength)
{
    tvecs = moveAxis(tvecs, rvecs, -SIDELENGTH, 0);
    tvecs = moveAxis(tvecs, rvecs, -SIDELENGTH, 1);
    tvecs = moveAxis(tvecs, rvecs, -SIDELENGTH, 2);
    switch (id)
    {
    case 1://cara 1
        rvecs = rotateXAxis(rvecs, -M_PI / 2);
        break;
    case 2://cara 2
        rvecs = rotateXAxis(rvecs, M_PI);
        break;
    case 3://cara 3
        rvecs = rotateYAxis(rvecs, M_PI / 2);
        rvecs = rotateZAxis(rvecs, M_PI);
        break;
    case 4://cara 4
        rvecs = rotateXAxis(rvecs, M_PI);
        rvecs = rotateYAxis(rvecs, -M_PI / 2);
        break;
    case 5://cara 5
        rvecs = rotateXAxis(rvecs, M_PI / 2);
        break;
    default://La cara 0 no precisa rotar
        break;
    }
}
\end{lstlisting}


\begin{figure}%
    \centering
    \subfloat[\centering Modelo del marcador fiduciario.]{{\includegraphics[width=6cm]{imaxes/cubo_marker.png} }}%
    \qquad
    \subfloat[\centering  Impresión del marcador fiduciario.]{{\includegraphics[width=6cm]{imaxes/cubo_marker_imp.png} }}%
    \caption{Diseño final del marcador fiduciario.}%
    \label{fig:cubo_marker}%
\end{figure}

En caso de que únicamente se detectase una cara del cubo, los vectores de rotación y translación finales serían los obtenidos llegados a este punto, pero es posible que se detecten hasta 3 caras en una única imagen. En estos casos se selecciona como resultado la media de los valores obtenidos en cada cara como se ve en la figura \figurename~\ref{fig:cubo_detected}.

\begin{figure}%
    \centering
    \subfloat[\centering Ejemplo de detección del cubo.]{{\includegraphics[width=6cm]{imaxes/cube_detected.png} }}%
    \qquad
    \subfloat[\centering Ejemplo de detección del cubo con oclusión.]{{\includegraphics[width=6cm]{imaxes/cube_detected_ocluded.png} }}%
    \caption{Imágenes sobre las que se estima la pose del cubo.}%
    \label{fig:cubo_detected}%
\end{figure}

Hasta ahora, se ha descrito el comportamiento de la aplicación para una única imagen. Sin embargo, aplicar este comportamiento a una fuente de vídeo apenas modifica el comportamiento. Una vez obtenidos los valores, simplemente hay que esperar a la siguiente imagen y volver a ejecutar el bucle de detección sobre la siguiente hasta que se desee terminar el programa. En la figura \figurename~\ref{fig:flujo_tfg} se puede apreciar el flujo aquí descrito.

\begin{figure}
  \centering
  \includesvg[width=16cm]{imaxes/flujo_tfg.svg}
  \caption{Diagrama de flujo de la solución de tracking.}
  \label{fig:flujo_tfg}
\end{figure}



\section{Implementación del Passthrough en Exposure Render}
Para la obtención de imágenes sobre las que poder trabajar se utilizaron las propias cámaras frontales del HTC Vive Pro 2. Se trata de un par de cámaras colocadas longitudinalmente a lo largo del frontal del casco, que permiten su uso en aplicaciones de realidad aumentada y realidad mixta. 
Para acceder a estas cámaras se debe hacer uso de SRworks C++ SDK.

Este software en el momento del desarrollo del trabajo, en su versión nativa, presentaba errores que imposibilitaron la reconstrucción de la imagen para ser visualizada en el casco.
\section{Integración del software de tracking en Exposure Render}
Durante la integración, se procuró mantener al mínimo la latencia introducida al analizar cada imagen. Por ello se implementó la solución como un thread. Este thread está sincronizado con la tasa de refresco de las cámaras; de forma que en cuanto una nueva imagen es recibida, se procesa, se devuelven los datos de seguimiento pertinentes, y esperan a la siguiente imagen.